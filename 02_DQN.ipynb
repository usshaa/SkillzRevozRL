{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a6c356a",
   "metadata": {},
   "source": [
    "\n",
    "# DQN (Deep Q-Network) with Stable Baselines3\n",
    "\n",
    "**Goal:** Train a DQN agent on `CartPole-v1`.  \n",
    "**Why DQN?** Extends Q-Learning to large/continuous state spaces by using a neural network to approximate Q(s,a), plus **experience replay** and a **target network** for stability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9c5112",
   "metadata": {},
   "source": [
    "\n",
    "### Why this step?  \n",
    "We install the required packages so the notebook can run anywhere (local, Colab).  \n",
    "- **gymnasium**: modern Gym API for RL environments  \n",
    "- **stable-baselines3**: popular, well-maintained RL algorithms (DQN, PPO, A2C)  \n",
    "- **tensorboard** (optional): view training curves  \n",
    "- **matplotlib**: simple plotting\n",
    "\n",
    "### How it works  \n",
    "`pip` installs packages into the active environment. In Colab, this writes to the session; locally, it writes to your venv.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c418bdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If running locally and already installed, you can skip this.\n",
    "# In Colab, keep this cell.\n",
    "# Note: remove the `-q` if you want to see full logs.\n",
    "!pip install -q gymnasium[classic-control]==0.29.1 stable-baselines3==2.3.2 tensorboard matplotlib numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9e6361",
   "metadata": {},
   "source": [
    "\n",
    "### Why this step?\n",
    "Import DQN components and create the environment.\n",
    "\n",
    "### How it works\n",
    "- `DQN` from SB3 wraps the algorithm.  \n",
    "- `evaluate_policy` runs several episodes without exploration to measure performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd6d8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "env\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b472eec6",
   "metadata": {},
   "source": [
    "\n",
    "### Why this step?\n",
    "Instantiate the DQN model and set important hyperparameters.\n",
    "\n",
    "### How it works\n",
    "- `MlpPolicy`: a small feed-forward network maps states → Q-values for each discrete action.  \n",
    "- `buffer_size`: replay buffer capacity.  \n",
    "- `exploration_fraction` & `exploration_final_eps`: ε schedule across training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b2f726",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = DQN(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=env,\n",
    "    learning_rate=1e-3,\n",
    "    buffer_size=50_000,\n",
    "    learning_starts=1000,\n",
    "    batch_size=64,\n",
    "    tau=1.0,\n",
    "    gamma=0.99,\n",
    "    train_freq=4,\n",
    "    gradient_steps=1,\n",
    "    target_update_interval=1000,\n",
    "    exploration_fraction=0.1,\n",
    "    exploration_final_eps=0.02,\n",
    "    verbose=1,\n",
    "    seed=42\n",
    ")\n",
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98979d50",
   "metadata": {},
   "source": [
    "\n",
    "### Why this step?\n",
    "Train the agent for a fixed number of timesteps.\n",
    "\n",
    "### How it works\n",
    "DQN will interact with the env, store transitions in replay, sample mini-batches, and periodically update the target network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d4d22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.learn(total_timesteps=100_000, log_interval=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e1970f",
   "metadata": {},
   "source": [
    "\n",
    "### Why this step?\n",
    "Evaluate the trained policy to estimate performance.\n",
    "\n",
    "### How it works\n",
    "`evaluate_policy` runs deterministic rollouts and returns mean ± std reward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301adb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10, deterministic=True)\n",
    "print(f\"Evaluation — Mean reward: {mean_reward:.2f} ± {std_reward:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8916126",
   "metadata": {},
   "source": [
    "\n",
    "### Why this step?\n",
    "Save and reload the model, then run an episode with **deterministic** actions.\n",
    "\n",
    "### How it works\n",
    "- `model.save` persists weights.  \n",
    "- `DQN.load` restores them.  \n",
    "- `predict(..., deterministic=True)` disables exploration noise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7feb27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.save(\"dqn_cartpole\")\n",
    "del model\n",
    "model = DQN.load(\"dqn_cartpole\", env=env)\n",
    "\n",
    "obs, _ = env.reset(seed=123)\n",
    "total_reward = 0.0\n",
    "for t in range(1000):\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "print(f\"Episode return (deterministic): {total_reward}\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
