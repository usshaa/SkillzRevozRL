{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b23d459a",
   "metadata": {},
   "source": [
    "\n",
    "# Comparison: Q-Learning vs DQN vs PPO vs A2C\n",
    "\n",
    "**Goal:** Provide a simple scaffold to compare training curves and evaluation scores.**  \n",
    "You can **train here** or **load saved models** from the other notebooks.\n",
    "\n",
    "> For apples-to-apples comparisons, keep seeds, environments, and timesteps aligned.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7071ce2c",
   "metadata": {},
   "source": [
    "\n",
    "### Why this step?  \n",
    "We install the required packages so the notebook can run anywhere (local, Colab).  \n",
    "- **gymnasium**: modern Gym API for RL environments  \n",
    "- **stable-baselines3**: popular, well-maintained RL algorithms (DQN, PPO, A2C)  \n",
    "- **tensorboard** (optional): view training curves  \n",
    "- **matplotlib**: simple plotting\n",
    "\n",
    "### How it works  \n",
    "`pip` installs packages into the active environment. In Colab, this writes to the session; locally, it writes to your venv.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1436797d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If running locally and already installed, you can skip this.\n",
    "# In Colab, keep this cell.\n",
    "# Note: remove the `-q` if you want to see full logs.\n",
    "!pip install -q gymnasium[classic-control]==0.29.1 stable-baselines3==2.3.2 tensorboard matplotlib numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62e5a0b",
   "metadata": {},
   "source": [
    "\n",
    "### Why this step?\n",
    "Import needed libraries and choose a common environment.\n",
    "\n",
    "### How it works\n",
    "We'll use `CartPole-v1` for the deep RL algorithms and `FrozenLake-v1` for tabular Q-learning (since CartPole has continuous observations).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34004ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from stable_baselines3 import DQN, PPO, A2C\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# Envs\n",
    "cartpole_env = gym.make(\"CartPole-v1\")\n",
    "frozen_env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7a13b7",
   "metadata": {},
   "source": [
    "\n",
    "### Why this step?\n",
    "(Option A) **Load** pre-trained models (if you've saved them).\n",
    "\n",
    "### How it works\n",
    "Uncomment the relevant lines to load models produced in earlier notebooks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a423d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dqn = DQN.load(\"dqn_cartpole\", env=cartpole_env)\n",
    "# ppo = PPO.load(\"ppo_cartpole\", env=cartpole_env)\n",
    "# a2c = A2C.load(\"a2c_cartpole\", env=cartpole_env)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f809f33c",
   "metadata": {},
   "source": [
    "\n",
    "### Why this step?\n",
    "(Option B) **Train** small models quickly just for demonstration.\n",
    "\n",
    "### How it works\n",
    "We keep timesteps low so the cell runs fast; for serious comparison, raise budgets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cecf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dqn = DQN(\"MlpPolicy\", cartpole_env, learning_rate=1e-3, buffer_size=20_000, learning_starts=500, verbose=0, seed=0)\n",
    "dqn.learn(total_timesteps=20_000)\n",
    "\n",
    "ppo = PPO(\"MlpPolicy\", cartpole_env, n_steps=128, batch_size=64, verbose=0, seed=0)\n",
    "ppo.learn(total_timesteps=20_000)\n",
    "\n",
    "a2c = A2C(\"MlpPolicy\", cartpole_env, n_steps=5, verbose=0, seed=0)\n",
    "a2c.learn(total_timesteps=20_000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4d9fd5",
   "metadata": {},
   "source": [
    "\n",
    "### Why this step?\n",
    "Evaluate all agents on the **same** environment with deterministic policies.\n",
    "\n",
    "### How it works\n",
    "We use 20 evaluation episodes for a quick but repeatable comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199a22ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_agent(model, env, n=20):\n",
    "    mean, std = evaluate_policy(model, env, n_eval_episodes=n, deterministic=True)\n",
    "    return mean, std\n",
    "\n",
    "dqn_m, dqn_s = eval_agent(dqn, cartpole_env, n=20)\n",
    "ppo_m, ppo_s = eval_agent(ppo, cartpole_env, n=20)\n",
    "a2c_m, a2c_s = eval_agent(a2c, cartpole_env, n=20)\n",
    "\n",
    "print(f\"DQN: {dqn_m:.1f} ± {dqn_s:.1f}\")\n",
    "print(f\"PPO: {ppo_m:.1f} ± {ppo_s:.1f}\")\n",
    "print(f\"A2C: {a2c_m:.1f} ± {a2c_s:.1f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce767b7",
   "metadata": {},
   "source": [
    "\n",
    "### Why this step?\n",
    "(For reference) Re-create **tabular Q-learning** quickly and evaluate on FrozenLake.\n",
    "\n",
    "### How it works\n",
    "Small helper to train a Q-table and compute mean return.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c38b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_q_learning(env, episodes=2000, alpha=0.1, gamma=0.99, eps=1.0, eps_min=0.01, eps_decay=0.995):\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    for ep in range(episodes):\n",
    "        s, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            a = env.action_space.sample() if np.random.rand() < eps else int(np.argmax(Q[s]))\n",
    "            ns, r, term, trunc, _ = env.step(a)\n",
    "            done = term or trunc\n",
    "            Q[s, a] += alpha * (r + gamma * np.max(Q[ns]) - Q[s, a])\n",
    "            s = ns\n",
    "        eps = max(eps_min, eps * eps_decay)\n",
    "    return Q\n",
    "\n",
    "Q = train_q_learning(frozen_env, episodes=2000)\n",
    "policy = np.argmax(Q, axis=1)\n",
    "\n",
    "def eval_q(policy, env, n=200):\n",
    "    total = 0.0\n",
    "    for i in range(n):\n",
    "        s, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            a = int(policy[s])\n",
    "            s, r, term, trunc, _ = env.step(a)\n",
    "            done = term or trunc\n",
    "            total += r\n",
    "    return total / n\n",
    "\n",
    "q_avg = eval_q(policy, frozen_env, n=500)\n",
    "print(f\"Q-Learning on FrozenLake — avg return over 500 eps: {q_avg:.3f}\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
