{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d65759ee",
   "metadata": {},
   "source": [
    "\n",
    "# A2C (Advantage Actor-Critic) with Stable Baselines3\n",
    "\n",
    "**Goal:** Train an A2C agent on `CartPole-v1`.  \n",
    "**Why A2C?** Simple, synchronous on-policy actor-critic; computes advantages and updates actor (policy) and critic (value) jointly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb6c70f",
   "metadata": {},
   "source": [
    "\n",
    "### Why this step?  \n",
    "We install the required packages so the notebook can run anywhere (local, Colab).  \n",
    "- **gymnasium**: modern Gym API for RL environments  \n",
    "- **stable-baselines3**: popular, well-maintained RL algorithms (DQN, PPO, A2C)  \n",
    "- **tensorboard** (optional): view training curves  \n",
    "- **matplotlib**: simple plotting\n",
    "\n",
    "### How it works  \n",
    "`pip` installs packages into the active environment. In Colab, this writes to the session; locally, it writes to your venv.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93fa61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If running locally and already installed, you can skip this.\n",
    "# In Colab, keep this cell.\n",
    "# Note: remove the `-q` if you want to see full logs.\n",
    "!pip install -q gymnasium[classic-control]==0.29.1 stable-baselines3==2.3.2 tensorboard matplotlib numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e22181",
   "metadata": {},
   "source": [
    "\n",
    "### Why this step?\n",
    "Import A2C and build vectorized environments to gather small rollouts frequently.\n",
    "\n",
    "### How it works\n",
    "A2C uses short trajectories (controlled by `n_steps`) to compute returns/advantages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819a92d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "vec_env = make_vec_env(\"CartPole-v1\", n_envs=4, seed=42)\n",
    "vec_env\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2c7717",
   "metadata": {},
   "source": [
    "\n",
    "### Why this step?\n",
    "Instantiate A2C and configure core hyperparameters.\n",
    "\n",
    "### How it works\n",
    "- `n_steps`: rollout length per env before each update  \n",
    "- `gamma`: discount factor  \n",
    "- `gae_lambda`: use GAE for variance reduction (SB3 enables it in A2C by default).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52178b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = A2C(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=vec_env,\n",
    "    n_steps=5,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=1.0,\n",
    "    ent_coef=0.0,\n",
    "    vf_coef=0.5,\n",
    "    max_grad_norm=0.5,\n",
    "    learning_rate=7e-4,\n",
    "    verbose=1,\n",
    "    seed=42,\n",
    ")\n",
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d1c224",
   "metadata": {},
   "source": [
    "\n",
    "### Why this step?\n",
    "Train A2C for a fixed number of timesteps.\n",
    "\n",
    "### How it works\n",
    "A2C gathers short on-policy rollouts, computes advantage estimates, and updates actor & critic synchronously.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22352b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.learn(total_timesteps=150_000, log_interval=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e721a3",
   "metadata": {},
   "source": [
    "\n",
    "### Why this step?\n",
    "Evaluate the learned policy.\n",
    "\n",
    "### How it works\n",
    "Deterministic evaluation on a single env to get mean ± std reward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1ca7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eval_env = gym.make(\"CartPole-v1\")\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "print(f\"A2C — Mean reward: {mean_reward:.2f} ± {std_reward:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6693dd20",
   "metadata": {},
   "source": [
    "\n",
    "### Why this step?\n",
    "(Optionally) Save/reload the model and run a quick rollout.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91291fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"a2c_cartpole\")\n",
    "del model\n",
    "model = A2C.load(\"a2c_cartpole\", env=vec_env)\n",
    "\n",
    "obs = vec_env.reset()   # no [0], since vec_env.reset() returns obs directly\n",
    "steps = 0\n",
    "while steps < 1000:\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, rewards, dones, infos = vec_env.step(action)   # only 4 values\n",
    "    steps += 1\n",
    "    if dones.any():   # reset if any env is done\n",
    "        obs = vec_env.reset()\n",
    "print(\"Completed a short deterministic rollout.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
