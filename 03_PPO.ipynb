{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77d5d301",
   "metadata": {},
   "source": [
    "\n",
    "# PPO (Proximal Policy Optimization) with Stable Baselines3\n",
    "\n",
    "**Goal:** Train a PPO agent on `CartPole-v1` with vectorized environments.  \n",
    "**Why PPO?** On-policy actor-critic with **clipped objective** to limit destructive policy updates — robust and widely used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1c68ae",
   "metadata": {},
   "source": [
    "\n",
    "### Why this step?  \n",
    "We install the required packages so the notebook can run anywhere (local, Colab).  \n",
    "- **gymnasium**: modern Gym API for RL environments  \n",
    "- **stable-baselines3**: popular, well-maintained RL algorithms (DQN, PPO, A2C)  \n",
    "- **tensorboard** (optional): view training curves  \n",
    "- **matplotlib**: simple plotting\n",
    "\n",
    "### How it works  \n",
    "`pip` installs packages into the active environment. In Colab, this writes to the session; locally, it writes to your venv.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b997559",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If running locally and already installed, you can skip this.\n",
    "# In Colab, keep this cell.\n",
    "# Note: remove the `-q` if you want to see full logs.\n",
    "!pip install -q gymnasium[classic-control]==0.29.1 stable-baselines3==2.3.2 tensorboard matplotlib numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6bad70",
   "metadata": {},
   "source": [
    "\n",
    "### Why this step?\n",
    "Import PPO and create **vectorized environments** to parallelize rollouts.\n",
    "\n",
    "### How it works\n",
    "`make_vec_env` spawns multiple env copies; PPO collects batches from all of them before each update.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6559c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "vec_env = make_vec_env(\"CartPole-v1\", n_envs=4, seed=42)\n",
    "vec_env\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcabe21",
   "metadata": {},
   "source": [
    "\n",
    "### Why this step?\n",
    "Instantiate the PPO model and set core hyperparameters.\n",
    "\n",
    "### How it works\n",
    "- `n_steps`: rollout length per env before an update  \n",
    "- `batch_size`: minibatch size per gradient step  \n",
    "- `clip_range`: PPO's policy ratio clipping parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858c63cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = PPO(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=vec_env,\n",
    "    n_steps=128,\n",
    "    batch_size=64,\n",
    "    n_epochs=10,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    clip_range=0.2,\n",
    "    ent_coef=0.0,\n",
    "    vf_coef=0.5,\n",
    "    max_grad_norm=0.5,\n",
    "    verbose=1,\n",
    "    seed=42,\n",
    ")\n",
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4972c1fc",
   "metadata": {},
   "source": [
    "\n",
    "### Why this step?\n",
    "Train PPO with parallel environments for a fixed budget.\n",
    "\n",
    "### How it works\n",
    "PPO alternates between collecting on-policy rollouts and performing several epochs of SGD on those data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660883b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.learn(total_timesteps=200_000, log_interval=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a648b7",
   "metadata": {},
   "source": [
    "\n",
    "### Why this step?\n",
    "Evaluate the trained PPO policy.\n",
    "\n",
    "### How it works\n",
    "We run `evaluate_policy` with deterministic actions on a non-vectorized env for clarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfc5429",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eval_env = gym.make(\"CartPole-v1\")\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "print(f\"PPO — Mean reward: {mean_reward:.2f} ± {std_reward:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf381165",
   "metadata": {},
   "source": [
    "\n",
    "### Why this step?\n",
    "(Optionally) Save and reload, then roll out one episode to verify behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe18388",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"ppo_cartpole\")\n",
    "del model\n",
    "model = PPO.load(\"ppo_cartpole\", env=vec_env)\n",
    "\n",
    "obs = vec_env.reset()\n",
    "ep_rewards = 0.0\n",
    "for _ in range(1000):\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, rewards, dones, infos = vec_env.step(action)   # only 4 values here\n",
    "    ep_rewards += float(rewards.mean())\n",
    "    if dones.any():   # at least one env finished\n",
    "        obs = vec_env.reset()\n",
    "print(\"Sample vectorized rollout mean-step reward:\", ep_rewards)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
