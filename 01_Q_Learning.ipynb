{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85375990",
   "metadata": {},
   "source": [
    "\n",
    "# Q-Learning (Tabular) — Discrete Environment\n",
    "\n",
    "**Goal:** Implement classic *tabular* Q-Learning on a simple discrete Gymnasium environment (e.g., FrozenLake).  \n",
    "We will build the Q-table, use ε-greedy exploration, apply the Bellman update, and evaluate the learned policy.\n",
    "\n",
    "**Key ideas:**  \n",
    "- Q-table `Q[s, a]` stores value estimates  \n",
    "- ε-greedy balances exploration vs exploitation  \n",
    "- Bellman update nudges Q toward `r + γ max_a' Q(s', a')`  \n",
    "- Works only for *small discrete* state spaces\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e2db57",
   "metadata": {},
   "source": [
    "\n",
    "### Why this step?  \n",
    "We install the required packages so the notebook can run anywhere (local, Colab).  \n",
    "- **gymnasium**: modern Gym API for RL environments  \n",
    "- **stable-baselines3**: popular, well-maintained RL algorithms (DQN, PPO, A2C)  \n",
    "- **tensorboard** (optional): view training curves  \n",
    "- **matplotlib**: simple plotting\n",
    "\n",
    "### How it works  \n",
    "`pip` installs packages into the active environment. In Colab, this writes to the session; locally, it writes to your venv.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2487e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If running locally and already installed, you can skip this.\n",
    "# In Colab, keep this cell.\n",
    "# Note: remove the `-q` if you want to see full logs.\n",
    "!pip install -q gymnasium[classic-control]==0.29.1 stable-baselines3==2.3.2 tensorboard matplotlib numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83a5026",
   "metadata": {},
   "source": [
    "\n",
    "### Why this step?\n",
    "Import dependencies and create a **discrete** environment so that we can index states/actions in a Q-table.\n",
    "\n",
    "### How it works\n",
    "- `gymnasium.make()` constructs the environment.  \n",
    "- We pick `FrozenLake-v1` (deterministic mode) to make convergence easier to observe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df78bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "n_states, n_actions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418316fc",
   "metadata": {},
   "source": [
    "\n",
    "### Why this step?\n",
    "Initialize the **Q-table** and hyperparameters that govern learning speed, discounting, exploration, and training duration.\n",
    "\n",
    "### How it works\n",
    "- `Q` starts at zeros.  \n",
    "- `alpha` (learning rate) controls how aggressively we update.  \n",
    "- `gamma` discounts future rewards.  \n",
    "- `epsilon` decays each episode to reduce exploration over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52564354",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Q = np.zeros((n_states, n_actions))\n",
    "\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "\n",
    "n_episodes = 3000\n",
    "max_steps = 200\n",
    "\n",
    "returns = []  # track episode rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fdbde5",
   "metadata": {},
   "source": [
    "\n",
    "### Why this step?\n",
    "Define ε-greedy **action selection** so we sometimes try random actions (exploration) rather than always choosing the current best (exploitation).\n",
    "\n",
    "### How it works\n",
    "If `rand < ε`: random action. Else: `argmax(Q[state])`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece1519c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def epsilon_greedy_action(Q, state, epsilon, action_space):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return action_space.sample()\n",
    "    return int(np.argmax(Q[state]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba166bb",
   "metadata": {},
   "source": [
    "\n",
    "### Why this step?\n",
    "Run the **training loop** across episodes and steps, applying the Q-Learning update.\n",
    "\n",
    "### How it works\n",
    "For each transition `(s, a, r, s')`, update:\n",
    "```\n",
    "Q[s,a] ← Q[s,a] + α [ r + γ * max_a' Q[s', a'] − Q[s,a] ]\n",
    "```\n",
    "We also decay `epsilon` each episode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873aa7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for ep in range(n_episodes):\n",
    "    state, _ = env.reset(seed=ep)\n",
    "    done = False\n",
    "    ep_return = 0.0\n",
    "\n",
    "    for t in range(max_steps):\n",
    "        action = epsilon_greedy_action(Q, state, epsilon, env.action_space)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Bellman target and update\n",
    "        best_next = np.max(Q[next_state])\n",
    "        td_target = reward + gamma * best_next\n",
    "        td_error = td_target - Q[state, action]\n",
    "        Q[state, action] += alpha * td_error\n",
    "\n",
    "        state = next_state\n",
    "        ep_return += reward\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    returns.append(ep_return)\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "len(returns), returns[-10:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98046339",
   "metadata": {},
   "source": [
    "\n",
    "### Why this step?\n",
    "Extract a **deterministic policy** from the learned `Q` and evaluate it without exploration.\n",
    "\n",
    "### How it works\n",
    "- Policy: `π(s) = argmax_a Q[s, a]`  \n",
    "- Run several evaluation episodes and compute mean reward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bc84e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "policy = np.argmax(Q, axis=1)\n",
    "print(\"Policy (state → action):\", policy)\n",
    "\n",
    "def evaluate(policy, env, n_eval_episodes=100):\n",
    "    total = 0.0\n",
    "    for i in range(n_eval_episodes):\n",
    "        s, _ = env.reset(seed=10_000 + i)\n",
    "        done = False\n",
    "        while not done:\n",
    "            a = int(policy[s])\n",
    "            s, r, terminated, truncated, _ = env.step(a)\n",
    "            done = terminated or truncated\n",
    "            total += r\n",
    "    return total / n_eval_episodes\n",
    "\n",
    "avg_ret = evaluate(policy, env, n_eval_episodes=200)\n",
    "print(f\"Average return over 200 eval episodes: {avg_ret:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e0206d",
   "metadata": {},
   "source": [
    "\n",
    "### Why this step?\n",
    "(Optional) **Plot** the training returns to visualize learning progress.\n",
    "\n",
    "### How it works\n",
    "We use `matplotlib` to draw the episode reward trajectory and a rolling mean.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed2086a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(returns, label=\"Episode return\")\n",
    "window = 50\n",
    "if len(returns) >= window:\n",
    "    mov = np.convolve(returns, np.ones(window)/window, mode='valid')\n",
    "    plt.plot(range(window-1, window-1+len(mov)), mov, label=f\"Moving avg ({window})\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.legend()\n",
    "plt.title(\"Q-Learning on FrozenLake (deterministic)\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
